import cv2
import numpy as np
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torchvision.models import efficientnet_b0, mobilenet_v3_small
import timm  # GhostNetì„ ìœ„í•´ í•„ìš”
import serial
import threading
import time
import json
import os
import boto3
from datetime import datetime, timezone
import pytz  # ì‹œê°„ëŒ€ ì„¤ì •ìš©
from collections import deque
import logging
from botocore.exceptions import ClientError
import warnings

# Warning ë©”ì‹œì§€ ì™„ì „íˆ ìˆ¨ê¸°ê¸°
warnings.filterwarnings("ignore")
import torchvision  # ì´ importê°€ warningì„ ë¯¸ë¦¬ ë°œìƒì‹œì¼œì„œ ë‚˜ì¤‘ì— ì•ˆ ë‚˜ì˜´

# ì„¤ì • íŒŒì¼ import
import config

class DoorBoxInferenceSystem:
    def __init__(self):
        # ë¡œê¹… ì„¤ì •
        self._setup_logging()
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        config.create_directories()
        
        # AWS S3 í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        self._setup_aws_client()
        
        # ê¸°ë³¸ ì„¤ì •
        self.rtsp_url = config.RTSP_URL
        self.serial_port = config.SERIAL_PORT
        self.serial_baudrate = config.SERIAL_BAUDRATE
        self.device_id = config.DEVICE_ID
        
        # êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”
        self.cap = None
        self.ser = None
        self.running = False
        
        # ëª¨ë¸ë“¤ ë¡œë“œ
        self._load_all_models()
        
        # ë¹„ë””ì˜¤ ë ˆì½”ë” ì´ˆê¸°í™”
        self._init_video_recorder()
        
        # S3 ì—…ë¡œë” ì´ˆê¸°í™”
        self._init_s3_uploader()
        
        # ìŠ¤ë ˆë“œ ê´€ë¦¬
        self.rtsp_thread = None
        self.serial_thread = None
        
        # í”„ë ˆì„ ë²„í¼
        self.latest_frame = None
        self.frame_lock = threading.Lock()
        
        # ê°ì§€ ìƒíƒœ ê´€ë¦¬
        self.detection_active = False
        self.first_detection_time = None
        self.last_capture_time = None
        self.detection_session_id = None
        
        self.logger.info("DoorBox ì¶”ë¡  ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(config.LOG_FILE),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        # ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œê¹… ë ˆë²¨ ì¡°ì •
        logging.getLogger('urllib3').setLevel(logging.WARNING)
        logging.getLogger('boto3').setLevel(logging.WARNING)
        logging.getLogger('botocore').setLevel(logging.WARNING)
        logging.getLogger('PIL').setLevel(logging.WARNING)
    
    def _setup_aws_client(self):
        """AWS S3 í´ë¼ì´ì–¸íŠ¸ ì„¤ì •"""
        self.s3_client = boto3.client(
            's3',
            aws_access_key_id=config.AWS_ACCESS_KEY_ID,
            aws_secret_access_key=config.AWS_SECRET_ACCESS_KEY,
            region_name=config.AWS_REGION
        )
        
        # S3 ì—°ê²° í…ŒìŠ¤íŠ¸
        try:
            self.s3_client.head_bucket(Bucket=config.AWS_BUCKET_NAME)
            self.logger.info(f"âœ… S3 ë²„í‚· ì—°ê²° í™•ì¸: {config.AWS_BUCKET_NAME}")
        except Exception as e:
            self.logger.error(f"S3 ë²„í‚· ì—°ê²° ì‹¤íŒ¨: {e}")
    
    def _load_all_models(self):
        """ëª¨ë“  ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ"""
        # 1. ê°ì • ë¶„ë¥˜ ëª¨ë¸ (EfficientNet-B0)
        self.emotion_model = self._load_emotion_model()
        self.emotion_transform = self._get_emotion_transform()
        
        # 2. ì•…ì„¸ì„œë¦¬(ë§ˆìŠ¤í¬) ë¶„ë¥˜ ëª¨ë¸ (GhostNet)
        self.accessory_model = self._load_ghostnet_model(config.ACCESSORY_MODEL_PATH, "ì•…ì„¸ì„œë¦¬")
        
        # 3. ì—°ë ¹ëŒ€ ë¶„ë¥˜ ëª¨ë¸ (EfficientNet-B0)
        self.age_model = self._load_efficientnet_model(config.AGE_MODEL_PATH, "ì—°ë ¹ëŒ€", num_classes=9)
        
        # 4. ì„±ë³„ ë¶„ë¥˜ ëª¨ë¸ (MobileNetV3-Small)
        self.gender_model = self._load_mobilenet_model(config.GENDER_MODEL_PATH, "ì„±ë³„")
        
        # ê³µí†µ ì „ì²˜ë¦¬ (224x224)
        self.common_transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
    
    def _load_emotion_model(self):
        """ê°ì • ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ (EfficientNet-B0)"""
        try:
            model = efficientnet_b0(pretrained=False)
            model.classifier = nn.Sequential(
                nn.Dropout(0.2),
                nn.Linear(1280, 2)  # negative, non-negative
            )
            
            checkpoint = torch.load(config.EMOTION_MODEL_PATH, map_location='cpu')
            model.load_state_dict(checkpoint)
            model.eval()
            
            self.logger.info("âœ… ê°ì • ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            return model
        except Exception as e:
            self.logger.error(f"ê°ì • ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _load_ghostnet_model(self, model_path, model_name):
        """GhostNet ëª¨ë¸ ë¡œë“œ (ì•…ì„¸ì„œë¦¬/ë§ˆìŠ¤í¬ ë¶„ë¥˜ìš©)"""
        try:
            if not os.path.exists(model_path):
                self.logger.warning(f"{model_name} ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_path}")
                return None
            
            # ì²´í¬í¬ì¸íŠ¸ ë¨¼ì € ë¡œë“œí•´ì„œ êµ¬ì¡° í™•ì¸
            checkpoint = torch.load(model_path, map_location='cpu')
            
            # state_dict ì¶”ì¶œ
            if 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
            elif 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            else:
                state_dict = checkpoint
            
            # classifier í¬ê¸° í™•ì¸
            classifier_weight_shape = None
            for key in state_dict.keys():
                if 'classifier.weight' in key:
                    classifier_weight_shape = state_dict[key].shape
                    break
            
            if classifier_weight_shape is not None:
                num_classes, feature_dim = classifier_weight_shape
                self.logger.info(f"{model_name} ëª¨ë¸ êµ¬ì¡°: {num_classes}í´ë˜ìŠ¤, {feature_dim}ì°¨ì›")
                
                # ì»¤ìŠ¤í…€ GhostNet ëª¨ë¸ ìƒì„± (feature_dimì— ë§ì¶°)
                class CustomGhostNet(nn.Module):
                    def __init__(self, num_classes=2, feature_dim=960):
                        super().__init__()
                        # GhostNet backbone (feature extractor only)
                        self.backbone = timm.create_model('ghostnet_100', pretrained=False, num_classes=0)
                        
                        # backbone ì¶œë ¥ ì°¨ì› í™•ì¸ ë° ì¡°ì •
                        backbone_dim = self.backbone.num_features
                        
                        # feature_dimì— ë§ì¶° projection layer ì¶”ê°€
                        if backbone_dim != feature_dim:
                            self.projection = nn.Linear(backbone_dim, feature_dim)
                        else:
                            self.projection = nn.Identity()
                        
                        # classifier
                        self.classifier = nn.Linear(feature_dim, num_classes)
                    
                    def forward(self, x):
                        features = self.backbone(x)
                        features = self.projection(features)
                        return self.classifier(features)
                
                model = CustomGhostNet(num_classes, feature_dim)
                
            else:
                # ê¸°ë³¸ ëª¨ë¸ ìƒì„±
                model = timm.create_model('ghostnet_100', pretrained=False, num_classes=2)
            
            # ëª¨ë¸ì— ë¡œë“œ (strict=Falseë¡œ í˜¸í™˜ì„± í™•ë³´)
            missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
            
            if missing_keys:
                self.logger.warning(f"ëˆ„ë½ëœ í‚¤: {len(missing_keys)}ê°œ")
            if unexpected_keys:
                self.logger.warning(f"ì˜ˆìƒì¹˜ ëª»í•œ í‚¤: {len(unexpected_keys)}ê°œ")
            
            model.eval()
            
            self.logger.info(f"âœ… {model_name} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (Custom GhostNet)")
            return model
            
        except Exception as e:
            self.logger.error(f"{model_name} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _load_efficientnet_model(self, model_path, model_name, num_classes=9):
        """EfficientNet-B0 ëª¨ë¸ ë¡œë“œ (ì—°ë ¹ëŒ€ ë¶„ë¥˜ìš©)"""
        try:
            if not os.path.exists(model_path):
                self.logger.warning(f"{model_name} ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_path}")
                return None
            
            # EfficientNet-B0 ëª¨ë¸ ìƒì„±
            model = efficientnet_b0(pretrained=False)
            model.classifier = nn.Sequential(
                nn.Dropout(0.2),
                nn.Linear(1280, num_classes)
            )
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            checkpoint = torch.load(model_path, map_location='cpu')
            
            # state_dict ì¶”ì¶œ
            if 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
            elif 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            else:
                state_dict = checkpoint
            
            # ëª¨ë¸ì— ë¡œë“œ
            model.load_state_dict(state_dict, strict=False)
            model.eval()
            
            self.logger.info(f"âœ… {model_name} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (EfficientNet-B0, {num_classes}í´ë˜ìŠ¤)")
            return model
            
        except Exception as e:
            self.logger.error(f"{model_name} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _load_mobilenet_model(self, model_path, model_name):
        """MobileNetV3-Small ëª¨ë¸ ë¡œë“œ (ì„±ë³„ ë¶„ë¥˜ìš©)"""
        try:
            if not os.path.exists(model_path):
                self.logger.warning(f"{model_name} ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_path}")
                return None
            
            # MobileNetV3-Small ëª¨ë¸ ìƒì„±
            model = mobilenet_v3_small(pretrained=False)
            model.classifier = nn.Sequential(
                nn.Linear(576, 1024),
                nn.Hardswish(),
                nn.Dropout(0.2),
                nn.Linear(1024, 2)  # male, female
            )
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            checkpoint = torch.load(model_path, map_location='cpu')
            
            # state_dict ì¶”ì¶œ
            if 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
            elif 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            else:
                state_dict = checkpoint
            
            # ëª¨ë¸ì— ë¡œë“œ
            model.load_state_dict(state_dict, strict=False)
            model.eval()
            
            self.logger.info(f"âœ… {model_name} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (MobileNetV3-Small)")
            return model
            
        except Exception as e:
            self.logger.error(f"{model_name} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _get_emotion_transform(self):
        """ê°ì • ë¶„ë¥˜ìš© ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        return transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((320, 320)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
    
    def _classify_all_models(self, face_crop):
        """ëª¨ë“  ëª¨ë¸ë¡œ ë¶„ë¥˜ ì‹¤í–‰"""
        results = {
            "emotion": None,
            "emotion_confidence": 0.0,
            "has_mask": None,
            "mask_confidence": 0.0,
            "gender": None,
            "gender_confidence": 0.0,
            "age_group": None,
            "age_confidence": 0.0
        }
        
        try:
            # 1. ì•…ì„¸ì„œë¦¬(ë§ˆìŠ¤í¬) ë¶„ë¥˜ - ìš°ì„  ì‹¤í–‰
            if self.accessory_model is not None:
                mask_result, mask_conf = self._classify_accessory(face_crop)
                results["has_mask"] = mask_result
                results["mask_confidence"] = mask_conf
                
                # ë§ˆìŠ¤í¬ ì°©ìš©ì‹œ ë‹¤ë¥¸ ë¶„ë¥˜ ê±´ë„ˆë›°ê¸°
                if mask_result:
                    self.logger.info("ë§ˆìŠ¤í¬ ì°©ìš© ê°ì§€ - ë‹¤ë¥¸ ë¶„ë¥˜ ìƒëµ")
                    return results
            
            # 2. ê°ì • ë¶„ë¥˜
            if self.emotion_model is not None:
                emotion, emotion_conf = self._classify_emotion(face_crop)
                results["emotion"] = emotion
                results["emotion_confidence"] = emotion_conf
            
            # 3. ì„±ë³„ ë¶„ë¥˜
            if self.gender_model is not None:
                gender, gender_conf = self._classify_gender(face_crop)
                results["gender"] = gender
                results["gender_confidence"] = gender_conf
            
            # 4. ì—°ë ¹ëŒ€ ë¶„ë¥˜
            if self.age_model is not None:
                age_group, age_conf = self._classify_age(face_crop)
                results["age_group"] = age_group
                results["age_confidence"] = age_conf
            
        except Exception as e:
            self.logger.error(f"ë¶„ë¥˜ ê³¼ì • ì˜¤ë¥˜: {e}")
        
        return results
    
    def _classify_emotion(self, face_crop):
        """ê°ì • ë¶„ë¥˜"""
        if self.emotion_model is None:
            return "unknown", 0.0
        
        try:
            input_tensor = self.emotion_transform(face_crop).unsqueeze(0)
            
            with torch.no_grad():
                outputs = self.emotion_model(input_tensor)
                probabilities = torch.softmax(outputs, dim=1)
                confidence, predicted = torch.max(probabilities, 1)
                
                emotion_classes = ["negative", "non-negative"]
                emotion = emotion_classes[predicted.item()]
                conf = confidence.item()
                
                return emotion, conf
        except Exception as e:
            self.logger.error(f"ê°ì • ë¶„ë¥˜ ì˜¤ë¥˜: {e}")
            return "unknown", 0.0
    
    def _classify_accessory(self, face_crop):
        """ì•…ì„¸ì„œë¦¬(ë§ˆìŠ¤í¬) ë¶„ë¥˜"""
        if self.accessory_model is None:
            return None, 0.0
        
        try:
            input_tensor = self.common_transform(face_crop).unsqueeze(0)
            
            with torch.no_grad():
                # backboneìœ¼ë¡œ feature ì¶”ì¶œ
                if hasattr(self.accessory_model, 'backbone'):
                    features = self.accessory_model.backbone(input_tensor)
                    
                    # ì°¨ì› ë§ì¶¤: 1280 â†’ 960
                    if features.shape[1] == 1280:
                        # ê°„ë‹¨í•œ ì°¨ì› ì¶•ì†Œ (ì²« 960ê°œ ì°¨ì›ë§Œ ì‚¬ìš©)
                        features = features[:, :960]
                    
                    # classifier ì ìš©
                    outputs = self.accessory_model.classifier(features)
                else:
                    # ì¼ë°˜ì ì¸ forward
                    outputs = self.accessory_model(input_tensor)
                
                probabilities = torch.softmax(outputs, dim=1)
                confidence, predicted = torch.max(probabilities, 1)
                
                # 0: ë§ˆìŠ¤í¬ ì—†ìŒ, 1: ë§ˆìŠ¤í¬ ìˆìŒ
                has_mask = bool(predicted.item())
                conf = confidence.item()
                
                # ì„ê³„ê°’ ì ìš© (ì‹ ë¢°ë„ê°€ 0.7 ì´ìƒì¼ ë•Œë§Œ ë§ˆìŠ¤í¬ ì°©ìš©ìœ¼ë¡œ íŒë‹¨)
                if conf < 0.7:
                    has_mask = False
                
                return has_mask, conf
        except Exception as e:
            self.logger.error(f"ì•…ì„¸ì„œë¦¬ ë¶„ë¥˜ ì˜¤ë¥˜: {e}")
            # ì—ëŸ¬ ë°œìƒì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            return False, 0.0
    
    def _classify_gender(self, face_crop):
        """ì„±ë³„ ë¶„ë¥˜"""
        if self.gender_model is None:
            return "unknown", 0.0
        
        try:
            input_tensor = self.common_transform(face_crop).unsqueeze(0)
            
            with torch.no_grad():
                outputs = self.gender_model(input_tensor)
                probabilities = torch.softmax(outputs, dim=1)
                confidence, predicted = torch.max(probabilities, 1)
                
                gender_classes = ["male", "female"]
                gender = gender_classes[predicted.item()]
                conf = confidence.item()
                
                return gender, conf
        except Exception as e:
            self.logger.error(f"ì„±ë³„ ë¶„ë¥˜ ì˜¤ë¥˜: {e}")
            return "unknown", 0.0
    
    def _classify_age(self, face_crop):
        """ì—°ë ¹ëŒ€ ë¶„ë¥˜ (EfficientNet-B0, 9í´ë˜ìŠ¤)"""
        if self.age_model is None:
            return "unknown", 0.0
        
        try:
            input_tensor = self.common_transform(face_crop).unsqueeze(0)
            
            with torch.no_grad():
                outputs = self.age_model(input_tensor)
                probabilities = torch.softmax(outputs, dim=1)
                confidence, predicted = torch.max(probabilities, 1)
                
                # 9ê°œ í´ë˜ìŠ¤ë¡œ í™•ì¥ (ì‹¤ì œ í´ë˜ìŠ¤ëª…ì€ ëª¨ë¸ì— ë”°ë¼ ì¡°ì •)
                age_classes = ["0-9", "10-19", "20-29", "30-39", "40-49", "50-59", "60-69", "70-79", "80+"]
                age_group = age_classes[predicted.item()]
                conf = confidence.item()
                
                return age_group, conf
        except Exception as e:
            self.logger.error(f"ì—°ë ¹ëŒ€ ë¶„ë¥˜ ì˜¤ë¥˜: {e}")
            return "unknown", 0.0
    
    def _init_video_recorder(self):
        """ë¹„ë””ì˜¤ ë ˆì½”ë” ì´ˆê¸°í™”"""
        self.frame_buffer = deque(maxlen=30 * config.VIDEO_CLIP_DURATION)  # 30fps * 5ì´ˆ
        self.recording = False
        self.buffer_thread = None
    
    def _start_video_buffering(self):
        """ë¹„ë””ì˜¤ í”„ë ˆì„ ë²„í¼ë§ ì‹œì‘"""
        self.recording = True
        
        def buffer_frames():
            while self.recording and self.running:
                with self.frame_lock:
                    if self.latest_frame is not None:
                        timestamp = time.time()
                        self.frame_buffer.append((self.latest_frame.copy(), timestamp))
                time.sleep(1/30)  # 30fps
        
        self.buffer_thread = threading.Thread(target=buffer_frames, daemon=True)
        self.buffer_thread.start()
    
    def _save_video_clip(self, detection_time, output_path):
        """5ì´ˆ ë¹„ë””ì˜¤ í´ë¦½ ì €ì¥"""
        if not self.frame_buffer:
            return False
        
        try:
            # ê°ì§€ ì‹œì  ê¸°ì¤€ìœ¼ë¡œ í”„ë ˆì„ í•„í„°ë§
            clip_frames = []
            start_time = detection_time - config.PRE_BUFFER_DURATION
            end_time = detection_time + config.POST_BUFFER_DURATION
            
            for frame, timestamp in self.frame_buffer:
                if start_time <= timestamp <= end_time:
                    clip_frames.append(frame)
            
            if len(clip_frames) < 10:
                return False
            
            # ë¹„ë””ì˜¤ íŒŒì¼ë¡œ ì €ì¥
            height, width = clip_frames[0].shape[:2]
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, 30.0, (width, height))
            
            for frame in clip_frames:
                out.write(frame)
            
            out.release()
            return True
            
        except Exception as e:
            self.logger.error(f"ë¹„ë””ì˜¤ ì €ì¥ ì˜¤ë¥˜: {e}")
            return False
    
    def _init_s3_uploader(self):
        """S3 ì—…ë¡œë” ì´ˆê¸°í™”"""
        self.upload_queue = []
        self.upload_running = False
        self.upload_thread = None
    
    def _start_s3_uploader(self):
        """S3 ì—…ë¡œë“œ ìŠ¤ë ˆë“œ ì‹œì‘"""
        def upload_worker():
            while self.upload_running:
                self._process_upload_batch()
                time.sleep(config.UPLOAD_INTERVAL)
        
        self.upload_running = True
        self.upload_thread = threading.Thread(target=upload_worker, daemon=True)
        self.upload_thread.start()
        self.logger.info("S3 ì—…ë¡œë“œ ìŠ¤ë ˆë“œ ì‹œì‘")
    
    def _queue_upload_data(self, frame_path, video_path, result_data):
        """S3 ì—…ë¡œë“œ íì— ë°ì´í„° ì¶”ê°€"""
        # ì„œìš¸ ì‹œê°„ëŒ€ë¡œ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
        seoul_tz = pytz.timezone('Asia/Seoul')
        timestamp = datetime.now(seoul_tz)
        
        upload_item = {
            'timestamp': timestamp,
            'frame_path': frame_path,
            'video_path': video_path,
            'result_data': result_data,
            'uploaded': False,
            'retry_count': 0
        }
        
        self.upload_queue.append(upload_item)
        self.logger.info(f"S3 í ì¶”ê°€ - ëŒ€ê¸°ì¤‘: {len(self.upload_queue)}ê°œ")
    
    def _generate_s3_paths(self, timestamp):
        """S3 ê²½ë¡œ ìƒì„± (ì„œìš¸ ì‹œê°„ëŒ€ ê¸°ì¤€)"""
        # timestampê°€ ì´ë¯¸ ì„œìš¸ ì‹œê°„ëŒ€ë©´ ê·¸ëŒ€ë¡œ, ì•„ë‹ˆë©´ ë³€í™˜
        if timestamp.tzinfo is None or timestamp.tzinfo.utcoffset(timestamp) is None:
            # naive datetimeì´ë©´ ì„œìš¸ ì‹œê°„ëŒ€ë¡œ ê°€ì •
            seoul_tz = pytz.timezone('Asia/Seoul')
            dt = seoul_tz.localize(timestamp)
        else:
            # timezone-aware datetimeì´ë©´ ì„œìš¸ ì‹œê°„ëŒ€ë¡œ ë³€í™˜
            seoul_tz = pytz.timezone('Asia/Seoul')
            dt = timestamp.astimezone(seoul_tz)
        
        folder_name = f"{dt.year:04d}{dt.month:02d}{dt.day:02d}_{dt.hour:02d}{dt.minute:02d}{dt.second:02d}_log"
        
        base_path = f"{dt.year:04d}/{dt.month:02d}/{dt.day:02d}/{folder_name}"
        file_prefix = f"{dt.year:04d}{dt.month:02d}{dt.day:02d}_{dt.hour:02d}{dt.minute:02d}{dt.second:02d}"
        
        return {
            'base_path': base_path,
            'frame_key': f"{base_path}/{file_prefix}_frame.jpg",
            'video_key': f"{base_path}/{file_prefix}_clip.mp4",
            'result_key': f"{base_path}/{file_prefix}_result.json"
        }
    
    def _upload_file_to_s3(self, local_path, s3_key, content_type):
        """ë‹¨ì¼ íŒŒì¼ S3 ì—…ë¡œë“œ"""
        try:
            with open(local_path, 'rb') as f:
                self.s3_client.put_object(
                    Bucket=config.AWS_BUCKET_NAME,
                    Key=s3_key,
                    Body=f,
                    ContentType=content_type
                )
            return True
        except Exception as e:
            self.logger.error(f"S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {s3_key}, ì˜¤ë¥˜: {e}")
            return False
    
    def _upload_json_to_s3(self, data, s3_key):
        """JSON ë°ì´í„° S3 ì—…ë¡œë“œ"""
        try:
            self.s3_client.put_object(
                Bucket=config.AWS_BUCKET_NAME,
                Key=s3_key,
                Body=json.dumps(data, ensure_ascii=False, indent=2),
                ContentType='application/json'
            )
            return True
        except Exception as e:
            self.logger.error(f"JSON ì—…ë¡œë“œ ì‹¤íŒ¨: {s3_key}, ì˜¤ë¥˜: {e}")
            return False
    
    def _process_upload_batch(self):
        """ë°°ì¹˜ ì—…ë¡œë“œ ì²˜ë¦¬"""
        if not self.upload_queue:
            return
        
        items_to_process = [item for item in self.upload_queue[:config.UPLOAD_BATCH_SIZE] 
                           if not item['uploaded'] and item['retry_count'] < 3]
        
        for item in items_to_process:
            try:
                s3_paths = self._generate_s3_paths(item['timestamp'])
                
                # ì´ë¯¸ì§€ í‚¤ ì„¤ì •
                item['result_data']['image_key'] = f"{config.AWS_BUCKET_NAME}/{s3_paths['frame_key']}"
                
                # 1. JSON ì—…ë¡œë“œ
                if self._upload_json_to_s3(item['result_data'], s3_paths['result_key']):
                    self.logger.info(f"âœ… JSON: {s3_paths['result_key']}")
                else:
                    item['retry_count'] += 1
                    continue
                
                # 2. í”„ë ˆì„ ì—…ë¡œë“œ
                if os.path.exists(item['frame_path']):
                    if self._upload_file_to_s3(item['frame_path'], s3_paths['frame_key'], 'image/jpeg'):
                        self.logger.info(f"âœ… í”„ë ˆì„: {s3_paths['frame_key']}")
                        os.remove(item['frame_path'])
                    else:
                        item['retry_count'] += 1
                        continue
                
                # 3. ë¹„ë””ì˜¤ ì—…ë¡œë“œ
                if item['video_path'] and os.path.exists(item['video_path']):
                    if self._upload_file_to_s3(item['video_path'], s3_paths['video_key'], 'video/mp4'):
                        self.logger.info(f"âœ… ë¹„ë””ì˜¤: {s3_paths['video_key']}")
                        os.remove(item['video_path'])
                    else:
                        item['retry_count'] += 1
                        continue
                
                item['uploaded'] = True
                self.logger.info(f"ì—…ë¡œë“œ ì™„ë£Œ: {s3_paths['base_path']}")
                
            except Exception as e:
                self.logger.error(f"ì—…ë¡œë“œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                item['retry_count'] += 1
        
        # ì™„ë£Œ/ì‹¤íŒ¨ ì•„ì´í…œ ì œê±°
        self.upload_queue = [item for item in self.upload_queue 
                           if not item['uploaded'] and item['retry_count'] < 3]
        
        if items_to_process:
            self.logger.info(f"ë°°ì¹˜ ì™„ë£Œ - ë‚¨ì€ í: {len(self.upload_queue)}ê°œ")
    
    def _detect_green_boxes(self, frame):
        """ì´ˆë¡ìƒ‰ ë°•ìŠ¤ ê²€ì¶œ"""
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        
        lower_green = np.array([40, 50, 50])
        upper_green = np.array([80, 255, 255])
        
        mask = cv2.inRange(hsv, lower_green, upper_green)
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        boxes = []
        for contour in contours:
            area = cv2.contourArea(contour)
            if area > 250 * 250:
                x, y, w, h = cv2.boundingRect(contour)
                boxes.append((x, y, w, h))
        
        return boxes
    
    def _expand_bbox(self, bbox, frame_shape, scale=1.5):
        """ë°”ìš´ë”© ë°•ìŠ¤ 1.5ë°° í™•ì¥"""
        x, y, w, h = bbox
        frame_h, frame_w = frame_shape[:2]
        
        center_x = x + w // 2
        center_y = y + h // 2
        
    def _expand_bbox(self, bbox, frame_shape, scale=1.5):
        """ë°”ìš´ë”© ë°•ìŠ¤ 1.5ë°° í™•ì¥"""
        x, y, w, h = bbox
        frame_h, frame_w = frame_shape[:2]
        
        center_x = x + w // 2
        center_y = y + h // 2
        
        new_w = int(w * scale)
        new_h = int(h * scale)
        
        new_x = max(0, center_x - new_w // 2)
        new_y = max(0, center_y - new_h // 2)
        
        new_x = min(new_x, frame_w - new_w)
        new_y = min(new_y, frame_h - new_h)
        new_w = min(new_w, frame_w - new_x)
        new_h = min(new_h, frame_h - new_y)
        
        return new_x, new_y, new_w, new_h
    
    def _save_detection_results(self, frame, classification_results):
        """ê°ì§€ ê²°ê³¼ ì €ì¥ ë° S3 í ì¶”ê°€"""
        try:
            # ì„œìš¸ ì‹œê°„ëŒ€ ì„¤ì •
            seoul_tz = pytz.timezone('Asia/Seoul')
            timestamp = datetime.now(seoul_tz)
            timestamp_str = timestamp.strftime("%Y%m%d_%H%M%S")
            
            frame_filename = f"{timestamp_str}_frame.jpg"
            video_filename = f"{timestamp_str}_clip.mp4"
            result_filename = f"{timestamp_str}_result.json"
            
            frame_path = os.path.join(config.LOCAL_FRAMES_DIR, frame_filename)
            video_path = os.path.join(config.LOCAL_VIDEOS_DIR, video_filename)
            result_path = os.path.join(config.LOCAL_RESULTS_DIR, result_filename)
            
            # 1. í”„ë ˆì„ ì €ì¥
            cv2.imwrite(frame_path, frame)
            self.logger.info(f"í”„ë ˆì„ ì €ì¥: {frame_filename}")
            
            # 2. 5ì´ˆ í´ë¦½ ì €ì¥
            detection_time = time.time()
            clip_saved = self._save_video_clip(detection_time, video_path)
            
            if clip_saved:
                self.logger.info(f"ë¹„ë””ì˜¤ ì €ì¥: {video_filename}")
            else:
                self.logger.warning("ë¹„ë””ì˜¤ ì €ì¥ ì‹¤íŒ¨")
                video_path = None
            
            # 3. JSON ë°ì´í„° ìƒì„± (ì„œìš¸ ì‹œê°„ëŒ€)
            result_data = {
                "day": timestamp.strftime("%Y%m%d"),
                "time": timestamp.strftime("%H:%M:%S"),
                "detection_results": {
                    "emotion": classification_results.get("emotion"),
                    "confidence": round(classification_results.get("emotion_confidence", 0.0), 3),
                    "has_mask": classification_results.get("has_mask"),
                    "gender": classification_results.get("gender"),
                    "age_group": classification_results.get("age_group")
                },
                "image_key": ""  # S3 ì—…ë¡œë“œì‹œ ì„¤ì •
            }
            
            # 4. ë¡œì»¬ì— JSON íŒŒì¼ ì €ì¥
            with open(result_path, 'w', encoding='utf-8') as f:
                json.dump(result_data, f, ensure_ascii=False, indent=2)
            self.logger.info(f"ê²°ê³¼ JSON ì €ì¥: {result_filename}")
            
            # 5. S3 ì—…ë¡œë“œ íì— ì¶”ê°€
            self._queue_upload_data(frame_path, video_path, result_data)
            
            # 6. ìƒì„¸ ë¡œê·¸ ì¶œë ¥ (ëª¨ë“  ë¶„ë¥˜ ê²°ê³¼ í‘œì‹œ)
            emotion = classification_results.get("emotion", "unknown")
            emotion_conf = classification_results.get("emotion_confidence", 0.0)
            has_mask = classification_results.get("has_mask")
            mask_conf = classification_results.get("mask_confidence", 0.0)
            gender = classification_results.get("gender", "unknown")
            gender_conf = classification_results.get("gender_confidence", 0.0)
            age_group = classification_results.get("age_group", "unknown")
            age_conf = classification_results.get("age_confidence", 0.0)
            
            # ë§ˆìŠ¤í¬ ìƒíƒœ í…ìŠ¤íŠ¸
            if has_mask is True:
                mask_text = f"ë§ˆìŠ¤í¬ ì°©ìš©({mask_conf:.3f})"
            elif has_mask is False:
                mask_text = f"ë§ˆìŠ¤í¬ ì—†ìŒ({mask_conf:.3f})"
            else:
                mask_text = "ë§ˆìŠ¤í¬ ë¯¸íŒë³„"
            
            self.logger.info("=== ë¶„ë¥˜ ê²°ê³¼ ìƒì„¸ ===")
            self.logger.info(f"   ê°ì •: {emotion} (ì‹ ë¢°ë„: {emotion_conf:.3f})")
            self.logger.info(f"   ë§ˆìŠ¤í¬: {mask_text}")
            self.logger.info(f"   ì„±ë³„: {gender} (ì‹ ë¢°ë„: {gender_conf:.3f})")
            self.logger.info(f"   ì—°ë ¹ëŒ€: {age_group} (ì‹ ë¢°ë„: {age_conf:.3f})")
            self.logger.info("========================")
            
        except Exception as e:
            self.logger.error(f"ê²°ê³¼ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def _rtsp_capture_worker(self):
        """RTSP ìº¡ì²˜ ìŠ¤ë ˆë“œ (ìµœì í™”)"""
        self.cap = cv2.VideoCapture(self.rtsp_url)
        
        if not self.cap.isOpened():
            self.logger.error("RTSP ì—°ê²° ì‹¤íŒ¨")
            return
        
        # RTSP ìŠ¤íŠ¸ë¦¼ ìµœì í™” ì„¤ì •
        self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # ë²„í¼ í¬ê¸° ìµœì†Œí™” (ì§€ì—° ì‹œê°„ ê°ì†Œ)
        self.cap.set(cv2.CAP_PROP_FPS, 30)        # FPS ì„¤ì •
        
        # ì´ˆê¸° ëª‡ í”„ë ˆì„ì€ ë²„ë¦¼ (ì—°ê²° ì•ˆì •í™”)
        for _ in range(5):
            self.cap.read()
        
        self.logger.info("RTSP ìŠ¤íŠ¸ë¦¼ ì‹œì‘ (ìµœì í™”ë¨)")
        
        frame_count = 0
        while self.running:
            ret, frame = self.cap.read()
            if ret:
                with self.frame_lock:
                    self.latest_frame = frame.copy()
                frame_count += 1
                
            else:
                self.logger.warning("í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨")
                time.sleep(0.01)  # ì§§ì€ ëŒ€ê¸°
            
            # CPU ì‚¬ìš©ë¥  ìµœì í™”
            time.sleep(0.001)  # 1ms ëŒ€ê¸° (ê¸°ì¡´ 0.1ì´ˆì—ì„œ ë‹¨ì¶•)
        
        self.cap.release()
        self.logger.info("RTSP ìº¡ì²˜ ì¢…ë£Œ")
    
    def _serial_reader_worker(self):
        """ì‹œë¦¬ì–¼ í†µì‹  ìŠ¤ë ˆë“œ (ìµœì í™”)"""
        try:
            self.ser = serial.Serial(self.serial_port, self.serial_baudrate, timeout=0.1)  # íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•
            self.logger.info(f"ì‹œë¦¬ì–¼ ì—°ê²° ì„±ê³µ: {self.serial_port}")
        except Exception as e:
            self.logger.error(f"ì‹œë¦¬ì–¼ ì—°ê²° ì‹¤íŒ¨: {e}")
            return
        
        while self.running:
            try:
                if self.ser.in_waiting > 0:
                    line = self.ser.readline().decode('utf-8').strip()
                    if line:
                        # YOLO ê°ì§€ ê²°ê³¼ë§Œ ì²˜ë¦¬ (ë¡œê·¸ ì¶œë ¥ ì•ˆí•¨)
                        if "[print_yolo_result]" in line and "[AI coordinate]" in line:
                            self._process_yolo_detection(line)
                        # ë‹¤ë¥¸ ì‹œë¦¬ì–¼ ë©”ì‹œì§€ëŠ” ë¬´ì‹œ
                
                time.sleep(0.001)  # 1ms ëŒ€ê¸° (ê¸°ì¡´ 0.1ì´ˆì—ì„œ ë‹¨ì¶•)
                
            except Exception as e:
                self.logger.error(f"ì‹œë¦¬ì–¼ ì½ê¸° ì˜¤ë¥˜: {e}")
                time.sleep(0.1)
        
        if self.ser:
            self.ser.close()
        self.logger.info("ì‹œë¦¬ì–¼ í†µì‹  ì¢…ë£Œ")
    
    def _process_yolo_detection(self, yolo_line):
        """YOLO ê°ì§€ ê²°ê³¼ ì²˜ë¦¬"""
        current_time = time.time()
        
        if not self.detection_active:
            # ìƒˆë¡œìš´ ê°ì§€ ì„¸ì…˜ ì‹œì‘
            self.detection_active = True
            self.first_detection_time = current_time
            self.detection_session_id = int(current_time)
            self.last_capture_time = None
            self.logger.info(f"ìƒˆë¡œìš´ ê°ì§€ ì„¸ì…˜ ì‹œì‘ (ID: {self.detection_session_id})")
            
            # ì²« ê°ì§€ í›„ ì¦‰ì‹œ ë˜ëŠ” ì§€ì—° í›„ ìº¡ì²˜ ì‹œì‘
            if config.DETECTION_DELAY > 0:
                threading.Timer(config.DETECTION_DELAY, self._start_periodic_capture).start()
                self.logger.info(f"{config.DETECTION_DELAY}ì´ˆ í›„ ìº¡ì²˜ ì‹œì‘ ì˜ˆì •")
            else:
                self._start_periodic_capture()
        
        # ê°ì§€ ì„¸ì…˜ íƒ€ì„ì•„ì›ƒ ì²´í¬
        if current_time - self.first_detection_time > config.DETECTION_TIMEOUT:
            self._end_detection_session()
    
    def _start_periodic_capture(self):
        """ì£¼ê¸°ì  ìº¡ì²˜ ì‹œì‘"""
        if self.detection_active:
            self.logger.info("ìº¡ì²˜ ë° ë¶„ë¥˜ ì‹œì‘")
            self._capture_and_process()
            
            # ë‹¤ìŒ ìº¡ì²˜ ìŠ¤ì¼€ì¤„ë§
            if self.detection_active:
                threading.Timer(config.CAPTURE_INTERVAL, self._start_periodic_capture).start()
    
    def _capture_and_process(self):
        """ìº¡ì²˜ ë° ì²˜ë¦¬ ì‹¤í–‰"""
        current_time = time.time()
        
        # ê°ì§€ ì„¸ì…˜ì´ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if not self.detection_active:
            return
        
        # ì„¸ì…˜ íƒ€ì„ì•„ì›ƒ ì²´í¬
        if current_time - self.first_detection_time > config.DETECTION_TIMEOUT:
            self._end_detection_session()
            return
        
        with self.frame_lock:
            if self.latest_frame is None:
                self.logger.warning("ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë ˆì„ì´ ì—†ìŒ")
                return
            current_frame = self.latest_frame.copy()
        
        # ì´ˆë¡ìƒ‰ ë°•ìŠ¤ ê²€ì¶œ
        boxes = self._detect_green_boxes(current_frame)
        
        if not boxes:
            self.logger.info("ì´ˆë¡ìƒ‰ ë°•ìŠ¤ ì—†ìŒ - ìº¡ì²˜ ê±´ë„ˆëœ€")
            return
        
        # ê°€ì¥ í° ë°•ìŠ¤ ì„ íƒ
        largest_box = max(boxes, key=lambda box: box[2] * box[3])
        
        # ë°•ìŠ¤ í™•ì¥
        expanded_box = self._expand_bbox(largest_box, current_frame.shape)
        x, y, w, h = expanded_box
        
        # ì–¼êµ´ ì˜ì—­ í¬ë¡­
        face_crop = current_frame[y:y+h, x:x+w]
        
        if face_crop.size == 0:
            self.logger.warning("í¬ë¡­ëœ ì–¼êµ´ ì˜ì—­ì´ ë¹„ì–´ìˆìŒ")
            return
        
        # ëª¨ë“  ëª¨ë¸ë¡œ ë¶„ë¥˜ ì‹¤í–‰
        classification_results = self._classify_all_models(face_crop)
        
        # ê²°ê³¼ ì €ì¥ ë° S3 ì—…ë¡œë“œ í ì¶”ê°€
        self._save_detection_results(current_frame, classification_results)
        
        self.last_capture_time = current_time
    
    def _end_detection_session(self):
        """ê°ì§€ ì„¸ì…˜ ì¢…ë£Œ"""
        if self.detection_active:
            self.logger.info(f"ê°ì§€ ì„¸ì…˜ ì¢…ë£Œ (ID: {self.detection_session_id})")
            self.detection_active = False
            self.first_detection_time = None
            self.last_capture_time = None
            self.detection_session_id = None
    
    def start(self):
        """ì‹œìŠ¤í…œ ì‹œì‘"""
        self.running = True
        
        # ë¹„ë””ì˜¤ ë²„í¼ë§ ì‹œì‘
        self._start_video_buffering()
        
        # S3 ì—…ë¡œë” ì‹œì‘
        self._start_s3_uploader()
        
        # RTSP ìº¡ì²˜ ìŠ¤ë ˆë“œ ì‹œì‘
        self.rtsp_thread = threading.Thread(target=self._rtsp_capture_worker, daemon=True)
        self.rtsp_thread.start()
        
        # ì‹œë¦¬ì–¼ ë¦¬ë” ìŠ¤ë ˆë“œ ì‹œì‘
        self.serial_thread = threading.Thread(target=self._serial_reader_worker, daemon=True)
        self.serial_thread.start()
        
        self.logger.info("DoorBox ì‹œìŠ¤í…œ ì‹œì‘ë¨")
        
        # ë©”ì¸ ë£¨í”„
        try:
            while self.running:
                time.sleep(1)
        except KeyboardInterrupt:
            self.logger.info("í‚¤ë³´ë“œ ì¸í„°ëŸ½íŠ¸ ê°ì§€")
            self.stop()
    
    def stop(self):
        """ì‹œìŠ¤í…œ ì¢…ë£Œ"""
        self.logger.info("ğŸ›‘ ì‹œìŠ¤í…œ ì¢…ë£Œ ì¤‘...")
        
        self.running = False
        
        # ë¹„ë””ì˜¤ ë ˆì½”ë” ì¢…ë£Œ
        self.recording = False
        if self.buffer_thread:
            self.buffer_thread.join(timeout=2)
        
        # S3 ì—…ë¡œë” ì¢…ë£Œ (ë‚¨ì€ í ì²˜ë¦¬)
        if self.upload_queue:
            self.logger.info("ë‚¨ì€ ì—…ë¡œë“œ ì²˜ë¦¬ ì¤‘...")
            self._process_upload_batch()
        
        self.upload_running = False
        if self.upload_thread:
            self.upload_thread.join(timeout=5)
        
        # ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°
        if self.rtsp_thread:
            self.rtsp_thread.join(timeout=3)
        if self.serial_thread:
            self.serial_thread.join(timeout=3)
        
        self.logger.info("âœ… DoorBox ì‹œìŠ¤í…œ ì¢…ë£Œ ì™„ë£Œ")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    doorbox = DoorBoxInferenceSystem()
    
    try:
        doorbox.start()
    except Exception as e:
        logging.error(f"ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
    finally:
        doorbox.stop()

if __name__ == "__main__":
    main()
