#!/usr/bin/env python3
"""
Door-Box Inference System (Raspberry Pi + CatchCAM)
ì´ˆë¡ìƒ‰ ë°•ìŠ¤ ìë™ ì¸ì‹ ë°©ì‹: ì¢Œí‘œ ë³€í™˜ ì—†ì´ ê·¸ë ¤ì§„ ë°•ìŠ¤ë¥¼ ì§ì ‘ ê²€ì¶œ, 1.5ë°° í™•ì¥ ver
"""

import cv2
import torch
import serial
import numpy as np
import json
import time
import threading
from datetime import datetime
import argparse
from pathlib import Path

class DoorBoxInference:
    def __init__(self, 
                 rtsp_url="rtsp://10.0.0.156/live1.sdp",
                 serial_port="/dev/ttyUSB0",
                 serial_baudrate=115200,
                 emotion_model_path="models/emotion2.pth",
                 output_dir="output",
                 save_visualization=True):
        
        self.rtsp_url = rtsp_url
        self.serial_port = serial_port
        self.serial_baudrate = serial_baudrate
        self.emotion_model_path = emotion_model_path
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.save_visualization = save_visualization
        
        # ìµœì‹  í”„ë ˆì„ ì €ì¥
        self.latest_frame = None
        self.frame_lock = threading.Lock()
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.emotion_model = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # ìƒíƒœ í”Œë˜ê·¸
        self.running = False
        
        # CatchCAM ì„¤ì •
        self.frame_width = 1920
        self.frame_height = 1080
        
        # ë””ë²„ê·¸ ëª¨ë“œ
        self.debug_mode = True

    def detect_green_boxes(self, frame):
        """í”„ë ˆì„ì—ì„œ ì´ˆë¡ìƒ‰ ë°•ìŠ¤ë“¤ì„ ê²€ì¶œ"""
        try:
            # HSV ìƒ‰ê³µê°„ìœ¼ë¡œ ë³€í™˜
            hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
            
            # ì´ˆë¡ìƒ‰ ë²”ìœ„ ì •ì˜
            green_ranges = [
                ((40, 100, 100), (80, 255, 255)),  # ë°ì€ ì´ˆë¡
                ((35, 150, 50), (85, 255, 255)),   # ì§„í•œ ì´ˆë¡
                ((45, 50, 50), (75, 255, 255)),    # ì—°í•œ ì´ˆë¡
            ]
            
            all_boxes = []
            
            for i, (lower, upper) in enumerate(green_ranges):
                lower_green = np.array(lower)
                upper_green = np.array(upper)
                
                # ì´ˆë¡ìƒ‰ ë§ˆìŠ¤í¬ ìƒì„±
                mask = cv2.inRange(hsv, lower_green, upper_green)
                
                # ë…¸ì´ì¦ˆ ì œê±°
                kernel = np.ones((3,3), np.uint8)
                mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
                mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
                
                # ìœ¤ê³½ì„  ê²€ì¶œ
                contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                
                for contour in contours:
                    # ìœ¤ê³½ì„ ì„ ì§ì‚¬ê°í˜•ìœ¼ë¡œ ê·¼ì‚¬
                    x, y, w, h = cv2.boundingRect(contour)
                    
                    # ë°•ìŠ¤ í¬ê¸° í•„í„°ë§ (ìµœì†Œ 250x250ìœ¼ë¡œ ë³€ê²½)
                    if 250 <= w <= 800 and 250 <= h <= 800:
                        aspect_ratio = w / h
                        if 0.3 <= aspect_ratio <= 2.0:
                            area = w * h
                            perimeter = cv2.arcLength(contour, True)
                            
                            if perimeter > 0:
                                rectangularity = (4 * np.pi * area) / (perimeter * perimeter)
                                if rectangularity > 0.2:
                                    all_boxes.append({
                                        'bbox': [x, y, x+w, y+h],
                                        'area': area,
                                        'aspect_ratio': aspect_ratio,
                                        'rectangularity': rectangularity,
                                        'range_index': i
                                    })
            
            # ì¤‘ë³µ ì œê±°
            filtered_boxes = self.remove_overlapping_boxes(all_boxes)
            
            if self.debug_mode and filtered_boxes:
                print(f"\nğŸŸ¢ ì´ˆë¡ìƒ‰ ë°•ìŠ¤ ê²€ì¶œ: {len(filtered_boxes)}ê°œ")
                for i, box in enumerate(filtered_boxes):
                    bbox = box['bbox']
                    print(f"   ë°•ìŠ¤{i+1}: {bbox} (í¬ê¸°: {bbox[2]-bbox[0]}x{bbox[3]-bbox[1]})")
            
            return filtered_boxes
            
        except Exception as e:
            print(f"âŒ ì´ˆë¡ìƒ‰ ë°•ìŠ¤ ê²€ì¶œ ì˜¤ë¥˜: {e}")
            return []
    
    def remove_overlapping_boxes(self, boxes):
        """ê²¹ì¹˜ëŠ” ë°•ìŠ¤ë“¤ ì œê±°"""
        if len(boxes) <= 1:
            return boxes
        
        boxes.sort(key=lambda x: x['area'], reverse=True)
        
        filtered = []
        for box1 in boxes:
            is_duplicate = False
            bbox1 = box1['bbox']
            
            for box2 in filtered:
                bbox2 = box2['bbox']
                iou = self.calculate_iou(bbox1, bbox2)
                if iou > 0.3:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                filtered.append(box1)
        
        return filtered
    
    def calculate_iou(self, box1, box2):
        """ë‘ ë°•ìŠ¤ì˜ IoU ê³„ì‚°"""
        x1_1, y1_1, x2_1, y2_1 = box1
        x1_2, y1_2, x2_2, y2_2 = box2
        
        xi1 = max(x1_1, x1_2)
        yi1 = max(y1_1, y1_2)
        xi2 = min(x2_1, x2_2)
        yi2 = min(y2_1, y2_2)
        
        if xi2 <= xi1 or yi2 <= yi1:
            return 0.0
        
        inter_area = (xi2 - xi1) * (yi2 - yi1)
        box1_area = (x2_1 - x1_1) * (y2_1 - y1_1)
        box2_area = (x2_2 - x1_2) * (y2_2 - y1_2)
        union_area = box1_area + box2_area - inter_area
        
        return inter_area / union_area if union_area > 0 else 0.0
    
    def select_best_face_box(self, green_boxes):
        """ì—¬ëŸ¬ ë°•ìŠ¤ ì¤‘ ê°€ì¥ í° ë°•ìŠ¤ ì„ íƒ (í¬ê¸° ìš°ì„ )"""
        if not green_boxes:
            return None
        
        if len(green_boxes) == 1:
            return green_boxes[0]
        
        # ë©´ì  ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ê°€ì¥ í° ê²ƒë¶€í„°)
        green_boxes.sort(key=lambda x: x['area'], reverse=True)
        best_box = green_boxes[0]
        
        if self.debug_mode:
            bbox = best_box['bbox']
            print(f"âœ… ê°€ì¥ í° ë°•ìŠ¤ ì„ íƒ: ë©´ì  {best_box['area']} (í¬ê¸°: {bbox[2]-bbox[0]}x{bbox[3]-bbox[1]})")
            if len(green_boxes) > 1:
                print(f"   ì „ì²´ {len(green_boxes)}ê°œ ë°•ìŠ¤ ì¤‘ì—ì„œ ì„ íƒ")
        
        return best_box

    def predict_emotion(self, face_image):
        """ê°ì • ë¶„ë¥˜ ì¶”ë¡  (direct_resize_imagenet ë°©ì‹ë§Œ ì‚¬ìš©)"""
        try:
            if self.debug_mode:
                print(f"\nğŸ§  ê°ì • ë¶„ë¥˜ ì‹œì‘ - ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°: {face_image.shape}")
            
            # direct_resize_imagenet ë°©ì‹ë§Œ ì‚¬ìš©
            face_320 = cv2.resize(face_image, (320, 320))
            face_tensor = self.preprocess_face_imagenet(face_320)
            
            if face_tensor is not None:
                with torch.no_grad():
                    outputs = self.emotion_model(face_tensor)
                    probabilities = torch.softmax(outputs, dim=1)
                    confidence = torch.max(probabilities).item()
                    predicted = torch.argmax(probabilities, dim=1).item()
                    
                    emotion_classes = ['negative', 'non-negative']
                    final_emotion = emotion_classes[predicted]
                    prob_list = probabilities[0].cpu().numpy().tolist()
                    
                    if self.debug_mode:
                        print(f"ğŸ“Š ê°ì • ë¶„ë¥˜ ê²°ê³¼ (direct_resize_imagenet):")
                        print(f"   {final_emotion} ({confidence:.3f})")
                        print(f"   í™•ë¥ ë¶„í¬: [neg:{prob_list[0]:.3f}, pos:{prob_list[1]:.3f}]")
                        
                        certainty = abs(confidence - 0.5)
                        if certainty > 0.1:
                            print(f"   â­ ë†’ì€ í™•ì‹ ë„! (ê²½ê³„ì„ ì—ì„œ {certainty:.3f} ë–¨ì–´ì§)")
                        else:
                            print(f"   âš ï¸ ë‚®ì€ í™•ì‹ ë„ (ê²½ê³„ì„ ì—ì„œ {certainty:.3f} ë–¨ì–´ì§)")
                    
                    return {
                        'emotion': final_emotion,
                        'confidence': confidence,
                        'probabilities': prob_list,
                        'method_used': 'direct_resize_imagenet',
                        'certainty': abs(confidence - 0.5)
                    }
            else:
                return {'emotion': 'unknown', 'confidence': 0.5, 'probabilities': [0.5, 0.5]}
                
        except Exception as e:
            print(f"ê°ì • ë¶„ë¥˜ ì˜¤ë¥˜: {e}")
            return None

    def resize_to_320_method1(self, face_image):
        """320x320 ë¦¬ì‚¬ì´ì¦ˆ ë°©ë²• 1: ì •ì‚¬ê°í˜• íŒ¨ë”© í›„ ë¦¬ì‚¬ì´ì¦ˆ"""
        h, w = face_image.shape[:2]
        if h != w:
            size = max(h, w)
            square_crop = np.zeros((size, size, 3), dtype=np.uint8)
            y_offset = (size - h) // 2
            x_offset = (size - w) // 2
            square_crop[y_offset:y_offset+h, x_offset:x_offset+w] = face_image
            face_square = square_crop
        else:
            face_square = face_image
        
        face_320 = cv2.resize(face_square, (320, 320), interpolation=cv2.INTER_LINEAR)
        return face_320
    
    def preprocess_face_imagenet(self, face_image_320):
        """ì „ì²˜ë¦¬: ImageNet ì •ê·œí™” (320x320 ì…ë ¥)"""
        try:
            if face_image_320.shape[:2] != (320, 320):
                face_image_320 = cv2.resize(face_image_320, (320, 320))
            
            face_rgb = cv2.cvtColor(face_image_320, cv2.COLOR_BGR2RGB)
            face_normalized = face_rgb.astype(np.float32) / 255.0
            mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)
            std = np.array([0.229, 0.224, 0.225], dtype=np.float32)
            face_normalized = (face_normalized - mean) / std
            face_transposed = np.transpose(face_normalized, (2, 0, 1))
            face_tensor = torch.from_numpy(face_transposed).unsqueeze(0).float().to(self.device)
            return face_tensor
        except Exception as e:
            print(f"ImageNet ì „ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return None
    
    def preprocess_face_simple(self, face_image_320):
        """ì „ì²˜ë¦¬: ë‹¨ìˆœ 0~1 ì •ê·œí™” (320x320 ì…ë ¥)"""
        try:
            if face_image_320.shape[:2] != (320, 320):
                face_image_320 = cv2.resize(face_image_320, (320, 320))
            
            face_rgb = cv2.cvtColor(face_image_320, cv2.COLOR_BGR2RGB)
            face_normalized = face_rgb.astype(np.float32) / 255.0
            face_transposed = np.transpose(face_normalized, (2, 0, 1))
            face_tensor = torch.from_numpy(face_transposed).unsqueeze(0).float().to(self.device)
            return face_tensor
        except Exception as e:
            print(f"ë‹¨ìˆœ ì „ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return None

    def load_emotion_model(self):
        """ê°ì •ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ"""
        try:
            print(f"ê°ì •ëª¨ë¸ ë¡œë”© ì¤‘: {self.emotion_model_path}")
            model_data = torch.load(self.emotion_model_path, map_location=self.device)
            if isinstance(model_data, dict):
                try:
                    import timm
                    self.emotion_model = timm.create_model('efficientnet_b0', num_classes=2, pretrained=False)
                    self.emotion_model.load_state_dict(model_data, strict=False)
                except ImportError:
                    print("timm ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. pip install timm ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
                    return False
            else:
                self.emotion_model = model_data
            self.emotion_model.to(self.device)
            self.emotion_model.eval()
            print("ê°ì •ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            return True
        except Exception as e:
            print(f"ê°ì •ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def setup_serial(self):
        """ì‹œë¦¬ì–¼ í†µì‹  ì„¤ì •"""
        try:
            print(f"ì‹œë¦¬ì–¼ í¬íŠ¸ ì—°ê²° ì‹œë„: {self.serial_port} @ {self.serial_baudrate} baud")
            self.serial_conn = serial.Serial(
                port=self.serial_port,
                baudrate=self.serial_baudrate,
                timeout=1
            )
            print(f"ì‹œë¦¬ì–¼ ì—°ê²° ì„±ê³µ: {self.serial_port}")
            return True
        except Exception as e:
            print(f"ì‹œë¦¬ì–¼ ì—°ê²° ì‹¤íŒ¨: {e}")
            return False
    
    def setup_rtsp(self):
        """RTSP ìŠ¤íŠ¸ë¦¼ ì„¤ì •"""
        try:
            print(f"RTSP ì—°ê²° ì‹œë„: {self.rtsp_url}")
            self.cap = cv2.VideoCapture(self.rtsp_url)
            if not self.cap.isOpened():
                print(f"RTSP ì—°ê²° ì‹¤íŒ¨: {self.rtsp_url}")
                return False
            fps = self.cap.get(cv2.CAP_PROP_FPS)
            width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            self.frame_width = width
            self.frame_height = height
            print(f"RTSP ì—°ê²° ì„±ê³µ: {width}x{height} @ {fps}fps")
            return True
        except Exception as e:
            print(f"RTSP ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
    
    def parse_yolo_detection(self, data_line):
        """CatchCAM YOLO detection ë°ì´í„° íŒŒì‹± (íŠ¸ë¦¬ê±° ìš©ë„, ë¡œê·¸ ìˆ¨ê¹€)"""
        try:
            if "[AI coordinate]" in data_line and "Count" in data_line:
                # ë¡œê·¸ ì¶œë ¥ ì œê±° (í¸ì˜ì„±ì„ ìœ„í•´)
                parts = data_line.split()
                count_idx = parts.index("Count") + 2
                count = int(parts[count_idx])
                if count > 0:
                    coords = {}
                    for i, part in enumerate(parts):
                        if part in ['score']:
                            if i + 2 < len(parts) and parts[i+1] == '=':
                                coords[part] = float(parts[i+2])
                    return {
                        'count': count,
                        'score': coords.get('score', 0.0),
                        'timestamp': time.time()
                    }
        except Exception as e:
            print(f"YOLO ë°ì´í„° íŒŒì‹± ì˜¤ë¥˜: {e}")
        return None

    def process_green_box_detection(self, frame, detection_score):
        """ì´ˆë¡ìƒ‰ ë°•ìŠ¤ ê²€ì¶œ ë° ê°ì •ë¶„ì„ (1.5ë°° í™•ì¥ í¬ë¡­)"""
        try:
            green_boxes = self.detect_green_boxes(frame)
            
            if not green_boxes:
                print("âŒ ì´ˆë¡ìƒ‰ ë°•ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return None, None, None, None, None
            
            best_box = self.select_best_face_box(green_boxes)
            
            if not best_box:
                print("âŒ ì ì ˆí•œ ì–¼êµ´ ë°•ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return None, None, None, None, None
            
            bbox = best_box['bbox']
            x1, y1, x2, y2 = bbox
            
            # ì›ë³¸ ë°•ìŠ¤ í¬ê¸° ê³„ì‚°
            original_width = x2 - x1
            original_height = y2 - y1
            
            # 1.5ë°° í™•ì¥ì„ ìœ„í•œ ìƒˆ í¬ê¸° ê³„ì‚°
            expand_factor = 1.5
            new_width = int(original_width * expand_factor)
            new_height = int(original_height * expand_factor)
            
            # ì¤‘ì•™ì  ê³„ì‚°
            center_x = (x1 + x2) // 2
            center_y = (y1 + y2) // 2
            
            # 1.5ë°° í™•ì¥ëœ ìƒˆ ì¢Œí‘œ ê³„ì‚°
            expanded_x1 = center_x - new_width // 2
            expanded_y1 = center_y - new_height // 2
            expanded_x2 = center_x + new_width // 2
            expanded_y2 = center_y + new_height // 2
            
            # í”„ë ˆì„ ê²½ê³„ ë‚´ë¡œ ì œí•œ
            expanded_x1 = max(0, expanded_x1)
            expanded_y1 = max(0, expanded_y1)
            expanded_x2 = min(self.frame_width, expanded_x2)
            expanded_y2 = min(self.frame_height, expanded_y2)
            
            # í™•ì¥ëœ ì˜ì—­ìœ¼ë¡œ í¬ë¡­
            face_crop = frame[expanded_y1:expanded_y2, expanded_x1:expanded_x2].copy()
            if face_crop.size == 0:
                print("âŒ í™•ì¥ëœ í¬ë¡­ ì˜ì—­ì´ ë¹„ì–´ìˆìŒ!")
                return None, None, None, None, None

            if self.debug_mode:
                print(f"ğŸ–¼ï¸  ì›ë³¸ ë°•ìŠ¤: {original_width}x{original_height}")
                print(f"ğŸ–¼ï¸  í™•ì¥ëœ í¬ë¡­: {face_crop.shape[1]}x{face_crop.shape[0]} (1.5ë°° í™•ì¥)")
                print(f"   ì›ë³¸ ì¢Œí‘œ: [{x1}, {y1}] -> [{x2}, {y2}]")
                print(f"   í™•ì¥ ì¢Œí‘œ: [{expanded_x1}, {expanded_y1}] -> [{expanded_x2}, {expanded_y2}]")

            emotion_result = self.predict_emotion(face_crop)

            result_frame = frame.copy()
            # ì›ë³¸ ì´ˆë¡ ë°•ìŠ¤ í‘œì‹œ (ì°¸ê³ ìš©)
            cv2.rectangle(result_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            # í™•ì¥ëœ í¬ë¡­ ì˜ì—­ í‘œì‹œ (ë¹¨ê°„ í…Œë‘ë¦¬)
            cv2.rectangle(result_frame, (expanded_x1, expanded_y1), (expanded_x2, expanded_y2), (0, 0, 255), 3)
            
            if emotion_result:
                certainty = emotion_result.get('certainty', 0)
                method = emotion_result.get('method_used', 'unknown')
                text = f"CROP: {emotion_result['emotion']} ({emotion_result['confidence']:.2f}) [1.5x]"
                if certainty > 0.1:
                    text += " â­"
            else:
                text = f"CROP (Score: {detection_score:.2f}) [1.5x]"
            
            (text_width, text_height), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
            y_text = max(0, expanded_y1 - 10)
            cv2.rectangle(result_frame, (expanded_x1, y_text-20), (expanded_x1+text_width+10, y_text+5), (0, 0, 255), -1)
            cv2.putText(result_frame, text, (expanded_x1+5, y_text-5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

            # í™•ì¥ëœ ì¢Œí‘œë¥¼ bboxë¡œ ë°˜í™˜ (ì €ì¥ìš©)
            expanded_bbox = [expanded_x1, expanded_y1, expanded_x2, expanded_y2]
            
            return result_frame, face_crop, emotion_result, expanded_bbox, best_box
        except Exception as e:
            print(f"âŒ ì´ˆë¡ ë°•ìŠ¤ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return None, None, None, None, None

    def save_result(self, frame, face_crop, bbox, emotion_result, result_frame, detection_score, box_info):
        """ê²°ê³¼ ì €ì¥"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
            result_data = {
                'timestamp': timestamp,
                'detection_score': float(detection_score),
                'bbox': bbox,
                'emotion': emotion_result if emotion_result else {
                    'emotion': 'unknown', 'confidence': 0.0, 'probabilities': []
                },
                'green_box_detection': {
                    'method': 'color_based_detection',
                    'box_area': box_info['area'],
                    'aspect_ratio': box_info['aspect_ratio'],
                    'rectangularity': box_info['rectangularity']
                },
                'image_files': {
                    'original': f"{timestamp}_original.jpg",
                    'face_crop': f"{timestamp}_face.jpg"
                },
                'frame_size': {'width': frame.shape[1], 'height': frame.shape[0]},
                'face_crop_size': {'width': face_crop.shape[1], 'height': face_crop.shape[0]}
            }
            
            if result_frame is not None and self.save_visualization:
                result_data['image_files']['result'] = f"{timestamp}_result.jpg"
                cv2.imwrite(str(self.output_dir / result_data['image_files']['result']), result_frame)
            
            cv2.imwrite(str(self.output_dir / result_data['image_files']['original']), frame)
            cv2.imwrite(str(self.output_dir / result_data['image_files']['face_crop']), face_crop)
            
            with open(self.output_dir / f"{timestamp}_result.json", 'w', encoding='utf-8') as f:
                json.dump(result_data, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {timestamp}")
            if emotion_result:
                print(f"   ê°ì •: {emotion_result['emotion']} ({emotion_result['confidence']:.2f})")
            print(f"   YOLO ì‹ ë¢°ë„: {detection_score:.3f}")
            print(f"   ê²€ì¶œëœ ë°•ìŠ¤: [{bbox[0]}, {bbox[1]}] -> [{bbox[2]}, {bbox[3]}]")
            print(f"   í¬ë¡­ í¬ê¸°: {face_crop.shape[1]}x{face_crop.shape[0]}  [green_box_detection]")
            return True
        except Exception as e:
            print(f"ê²°ê³¼ ì €ì¥ ì˜¤ë¥˜: {e}")
            return False

    def serial_reader_thread(self):
        """ì‹œë¦¬ì–¼ ë°ì´í„° ì½ê¸° ìŠ¤ë ˆë“œ (2ì´ˆ ê°„ê²© + 2ì´ˆ ì•ˆì •í™” ì‹œê°„)"""
        print("ì‹œë¦¬ì–¼ ë¦¬ë” ìŠ¤ë ˆë“œ ì‹œì‘ - YOLO ê²€ì¶œ íŠ¸ë¦¬ê±° ëŒ€ê¸°")
        last_detection_time = 0
        min_detection_interval = 2.0  # 2ì´ˆë¡œ ë³€ê²½
        first_detection_time = None  # ì²« ê²€ì¶œ ì‹œê°„ ê¸°ë¡
        stabilization_period = 2.0   # 2ì´ˆ ì•ˆì •í™” ì‹œê°„
        
        while self.running:
            try:
                if self.serial_conn.in_waiting > 0:
                    line = self.serial_conn.readline().decode('utf-8', errors='ignore').strip()
                    if line:
                        detection_data = self.parse_yolo_detection(line)
                        if detection_data:
                            current_time = time.time()
                            
                            # ì²« ê²€ì¶œì¸ì§€ í™•ì¸
                            if first_detection_time is None:
                                first_detection_time = current_time
                                print(f"\nğŸ¯ ì²« YOLO ê²€ì¶œ! ì•ˆì •í™”ë¥¼ ìœ„í•´ {stabilization_period}ì´ˆ ëŒ€ê¸°...")
                                continue
                            
                            # ì•ˆì •í™” ì‹œê°„ì´ ì§€ë‚¬ëŠ”ì§€ í™•ì¸
                            if current_time - first_detection_time < stabilization_period:
                                remaining_time = stabilization_period - (current_time - first_detection_time)
                                print(f"â³ ì•ˆì •í™” ëŒ€ê¸° ì¤‘... (ë‚¨ì€ ì‹œê°„: {remaining_time:.1f}ì´ˆ)")
                                continue
                            
                            # ìµœì†Œ ê²€ì¶œ ê°„ê²© í™•ì¸
                            if current_time - last_detection_time < min_detection_interval:
                                continue
                            
                            print(f"\n{'='*60}")
                            print(f"ğŸ¯ YOLO íŠ¸ë¦¬ê±°! ì´ˆë¡ ë°•ìŠ¤ ê²€ì¶œ ì‹œì‘ (250x250 ìµœì†Œ í¬ê¸°)")
                            print(f"   ì‹ ë¢°ë„: {detection_data['score']:.3f}")
                            
                            current_frame = None
                            with self.frame_lock:
                                if self.latest_frame is not None:
                                    current_frame = self.latest_frame.copy()
                            
                            if current_frame is not None:
                                result = self.process_green_box_detection(
                                    current_frame, detection_data['score']
                                )
                                
                                if result and len(result) == 5:
                                    result_frame, face_crop, emotion_result, bbox, box_info = result
                                    if face_crop is not None:
                                        saved = self.save_result(
                                            current_frame, face_crop, bbox, emotion_result,
                                            result_frame, detection_data['score'], box_info
                                        )
                                        if saved:
                                            last_detection_time = current_time
                                            print(f"{'='*60}\n")
                            else:
                                print("â³ í”„ë ˆì„ì´ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•ŠìŒ")
                        elif "[SEND]" in line or "[RECV]" in line:
                            if "inf_number" not in line:
                                print(f"Serial: {line}")
            except Exception as e:
                print(f"ì‹œë¦¬ì–¼ ì½ê¸° ì˜¤ë¥˜: {e}")
                time.sleep(0.1)
    
    def frame_capture_thread(self):
        """RTSP í”„ë ˆì„ ìº¡ì²˜ ìŠ¤ë ˆë“œ"""
        print("í”„ë ˆì„ ìº¡ì²˜ ìŠ¤ë ˆë“œ ì‹œì‘")
        frame_count = 0
        error_count = 0
        max_errors = 10
        
        while self.running:
            try:
                ret, frame = self.cap.read()
                if ret:
                    with self.frame_lock:
                        self.latest_frame = frame
                    frame_count += 1
                    error_count = 0
                    if frame_count % 300 == 0:
                        print(f"ğŸ“¹ í”„ë ˆì„ ìº¡ì²˜ ì¤‘... (ì´ {frame_count} í”„ë ˆì„)")
                else:
                    error_count += 1
                    if error_count >= max_errors:
                        print(f"âš ï¸  í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨ {error_count}íšŒ - ì¬ì—°ê²° ì‹œë„")
                        self.reconnect_rtsp()
                        error_count = 0
                    time.sleep(0.1)
            except Exception as e:
                print(f"í”„ë ˆì„ ìº¡ì²˜ ì˜¤ë¥˜: {e}")
                time.sleep(0.1)
    
    def reconnect_rtsp(self):
        """RTSP ì¬ì—°ê²°"""
        try:
            if hasattr(self, 'cap'):
                self.cap.release()
            time.sleep(1)
            self.setup_rtsp()
        except Exception as e:
            print(f"RTSP ì¬ì—°ê²° ì‹¤íŒ¨: {e}")
    
    def run(self):
        """ë©”ì¸ ì‹¤í–‰ ë£¨í”„"""
        print("\n" + "="*60)
        print("ğŸšª Door-Box ì¸í¼ëŸ°ìŠ¤ ì‹œìŠ¤í…œ (ì´ˆë¡ ë°•ìŠ¤ ê²€ì¶œ)")
        print("   YOLO íŠ¸ë¦¬ê±° â†’ ì´ˆë¡ ë°•ìŠ¤ ìë™ ê²€ì¶œ â†’ ê°ì • ë¶„ë¥˜")
        print("   ì¢Œí‘œ ë³€í™˜ ì—†ì´ ê·¸ë ¤ì§„ ë°•ìŠ¤ë¥¼ ì§ì ‘ ì¸ì‹!")
        print("="*60 + "\n")
        
        if not self.load_emotion_model():
            return False
        if not self.setup_serial():
            return False
        if not self.setup_rtsp():
            return False
        
        self.running = True
        
        serial_thread = threading.Thread(target=self.serial_reader_thread, daemon=True)
        frame_thread = threading.Thread(target=self.frame_capture_thread, daemon=True)
        serial_thread.start()
        frame_thread.start()
        
        print("\nâœ… ëª¨ë“  ìŠ¤ë ˆë“œ ì‹œì‘ ì™„ë£Œ")
        print("ğŸ” YOLO íŠ¸ë¦¬ê±° ëŒ€ê¸° ì¤‘... (ì¢…ë£Œ: Ctrl+C)\n")
        
        try:
            while self.running:
                time.sleep(0.1)
        except KeyboardInterrupt:
            print("\n\nì¢…ë£Œ ì‹ í˜¸ ë°›ìŒ...")
        finally:
            self.cleanup()
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        print("ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        self.running = False
        if hasattr(self, 'serial_conn'):
            try:
                self.serial_conn.close()
            except Exception:
                pass
        if hasattr(self, 'cap'):
            try:
                self.cap.release()
            except Exception:
                pass
        cv2.destroyAllWindows()
        print("âœ… ì •ë¦¬ ì™„ë£Œ\n")

def main():
    parser = argparse.ArgumentParser(description='Door-Box ì´ˆë¡ ë°•ìŠ¤ ê²€ì¶œ ì‹œìŠ¤í…œ')
    parser.add_argument('--rtsp_url', default='rtsp://10.0.0.156/live1.sdp', help='CatchCAM RTSP URL')
    parser.add_argument('--serial_port', default='/dev/ttyUSB0', help='ì‹œë¦¬ì–¼ í¬íŠ¸')
    parser.add_argument('--serial_baudrate', type=int, default=115200, help='ì‹œë¦¬ì–¼ í†µì‹  ì†ë„')
    parser.add_argument('--emotion_model', required=True, help='ê°ì •ë¶„ë¥˜ ëª¨ë¸ ê²½ë¡œ (.pth íŒŒì¼)')
    parser.add_argument('--output_dir', default='output', help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--no-visualization', action='store_true', help='ì‹œê°í™” ì´ë¯¸ì§€ ì €ì¥ ì•ˆí•¨')
    args = parser.parse_args()
    
    doorbox = DoorBoxInference(
        rtsp_url=args.rtsp_url,
        serial_port=args.serial_port,
        serial_baudrate=args.serial_baudrate,
        emotion_model_path=args.emotion_model,
        output_dir=args.output_dir,
        save_visualization=not args.no_visualization
    )
    doorbox.run()

if __name__ == "__main__":
    main()
